{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib import image\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "mat_data = scipy.io.loadmat('data/ex4data1.mat')\n",
    "mat_weights = scipy.io.loadmat('data/ex4weights.mat')\n",
    "\n",
    "X = mat_data['X']\n",
    "y = mat_data['y']\n",
    "weights_course = np.concatenate([mat_weights['Theta1'].flatten(),\n",
    "                                mat_weights['Theta2'].flatten()],\n",
    "                                axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "    Instance a neural network with user-defined configuration \n",
    "    and sigmoid activation function. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    layers_size : a list containing numbers of artificial neurons\n",
    "                  in each layer including output and input layers.\n",
    "    X_dat : a training set features of type pandas or numpy.array.\n",
    "    y_data: labels of each training example\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, X_data, y_data, layers_size):\n",
    "        self.X = X_data if type(X_data) == np.ndarray\\\n",
    "                    else np.array(X_data, dtype = 'float32')\n",
    "        self.y = y_data if type(y_data) == np.ndarray\\\n",
    "                    else np.array(y_data)\n",
    "        self.labels = np.unique(self.y)\n",
    "        self.y_mat = (self.labels == self.y) * 1\n",
    "        self.lsize = layers_size\n",
    "        self.net_input = None                                   # list of input data for each layer except the first one\n",
    "        self.net_output = None                                  # list of output data of each layer\n",
    "        self.net_weights_trained = None                         # flatten array of trained weights\n",
    "        \n",
    "    def AddBias(self, x, value):\n",
    "        if x.ndim > 1:\n",
    "            return np.insert(x, 0, value, axis = 1)\n",
    "        return np.insert(x, 0, value, axis = 0)\n",
    "    \n",
    "    def Sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def SigmoidGrad(self, z):\n",
    "        return self.Sigmoid(z) * (1 - self.Sigmoid(z))\n",
    "    \n",
    "    def RandomWeights(self):\n",
    "        \"\"\"\n",
    "        Create an array of random weights lying in the range\n",
    "        [-eps, + eps], where choosing of eps refers to Xavier \n",
    "        weight initialization.  \n",
    "        \n",
    "        \"\"\"\n",
    "        nlayers = len(self.lsize)\n",
    "        res = np.array([])\n",
    "        for i in range(nlayers - 1):\n",
    "            clsize = self.lsize[i] + 1\n",
    "            plsize = self.lsize[i+1]\n",
    "            eps = np.sqrt(6)/np.sqrt(clsize + plsize)\n",
    "            weights = np.random.rand(clsize * plsize) \n",
    "            weights = weights * 2 * eps - eps\n",
    "            res = np.concatenate((res, weights.flatten()),\n",
    "                                axis = 0) \n",
    "            \n",
    "        return res\n",
    "    \n",
    "    def Weights(self, net_weights, layer):\n",
    "        \"\"\"\"\n",
    "        Reshape a slice of flatten weights array to get \n",
    "        a weight matrix between layer and layer+1.\n",
    "        \n",
    "        \"\"\"\n",
    "        left = 0\n",
    "        right = 0\n",
    "        for i in range(layer):\n",
    "            clsize = self.lsize[i+1]\n",
    "            plsize = self.lsize[i] + 1\n",
    "            right += clsize * plsize\n",
    "            if i == layer-1:\n",
    "                weights = net_weights[left:right]\n",
    "            left += plsize * clsize\n",
    "            \n",
    "        return weights.reshape(clsize, plsize).T\n",
    "    \n",
    "    def ForwardPropagation(self, net_weights, X):  \n",
    "        \"\"\"\n",
    "        Propagate X data through the network to get activation\n",
    "        values of each units; then assign this values to class\n",
    "        variables.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        net_weights : flatten array of all weights of network\n",
    "        \n",
    "        \"\"\"\n",
    "        net_in = []                                                     \n",
    "        net_out = []   \n",
    "        \n",
    "        N = len(self.lsize)\n",
    "        for layer in range(N):\n",
    "            if layer == 0:\n",
    "                out = self.AddBias(X, 1)\n",
    "                net_out.append(out)\n",
    "            else:\n",
    "                weights = self.Weights(net_weights, layer)\n",
    "                inp = np.matmul(net_out[-1], weights)\n",
    "                if layer == N - 1:\n",
    "                    out = self.Sigmoid(inp)\n",
    "                else:\n",
    "                    out = self.AddBias(self.Sigmoid(inp), 1)\n",
    "                net_in.append(inp)\n",
    "                net_out.append(out)\n",
    "                \n",
    "        self.net_input = net_in\n",
    "        self.net_output = net_out\n",
    "    \n",
    "    def CostFunction(self, net_weights, X, y, lmbda):\n",
    "        \"\"\"\n",
    "        Calculate a logarithmic cost function with regularization \n",
    "        given weights and input data. This function returns a real\n",
    "        number.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        net_weights : flatten array of all weights of network\n",
    "        X, y : data to calculate cost\n",
    "        lmbda : regularization parameter\n",
    "        \n",
    "        \"\"\"\n",
    "        self.ForwardPropagation(net_weights, X)\n",
    "\n",
    "        m = X.shape[0] \n",
    "        \n",
    "        out = self.net_output[-1]\n",
    "        nonreg_term = np.sum(-y * np.log(out)\\\n",
    "                             - (1 - y) * np.log(1 - out))\n",
    "        reg_term = [np.sum(self.Weights(net_weights, layer)[:,1:]**2) \n",
    "                    for layer in range(1, len(self.lsize))]\n",
    "        \n",
    "        return nonreg_term/m + lmbda*sum(reg_term)/(2*m)\n",
    "    \n",
    "    def BackwardPropagation(self, net_weights, X, y, lmbda):\n",
    "        \"\"\"\n",
    "        Propagate backward the output error using true labels. \n",
    "        Output of the first layer is assumed to be error-free.\n",
    "        This function returns a vector of partial derivatives\n",
    "        of the neural network weights.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        net_weights : flatten array of all weights of network\n",
    "        X, y : data to calculate cost\n",
    "        lmbda : regularization parameter\n",
    "        \n",
    "        \"\"\"\n",
    "        net_delta = []\n",
    "        net_grad = []\n",
    "        \n",
    "        m = X.shape[0] \n",
    "\n",
    "        N = len(self.lsize)\n",
    "        for layer in range(N-1, 0, -1):\n",
    "            if layer == N - 1:\n",
    "                delta = self.net_output[layer] - y\n",
    "            else:\n",
    "                weights = self.Weights(net_weights, layer+1)\n",
    "                inp = self.net_input[layer-1]\n",
    "                grad = self.SigmoidGrad(inp)\n",
    "                delta = np.matmul(delta, weights.T[:,1:]) * grad\n",
    "            net_delta.append(delta)\n",
    "            \n",
    "            out = self.net_output[layer-1]\n",
    "            nonreg_term = np.matmul(delta.T, out)\n",
    "            reg_term = self.AddBias(self.Weights(net_weights, \n",
    "                                                    layer)[:,1:], 0)\n",
    "            net_grad.insert(0, nonreg_term/m + lmbda*reg_term.T/m)\n",
    "            \n",
    "        return np.concatenate([i.flatten() for i in net_grad],\n",
    "                              axis = 0)       \n",
    "        \n",
    "    def Fit(self, lmbda):\n",
    "        \"\"\"\n",
    "        Fit the model by minimizing the cost function using TNC\n",
    "        method. It doesn't always minimize correct, but on the \n",
    "        data from Andrew Ng course fit function works quite well.\n",
    "        For shuffled data this neural network provide bad results.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        lmbda : regularization parameter.\n",
    "        \n",
    "        \"\"\"\n",
    "        random_weights = self.RandomWeights()\n",
    "        y_mat = (self.labels == self.y) * 1\n",
    "        args_ = (self.X, self.y_mat, lmbda)\n",
    "        \n",
    "        res = minimize(fun = self.CostFunction, args = args_, \n",
    "                       x0 = random_weights, method = 'TNC',\n",
    "                       jac = self.BackwardPropagation)\n",
    "        self.net_weights_trained = res.x\n",
    "        \n",
    "        print(\"The model was trained\")\n",
    "        \n",
    "    def Predict(self, X):\n",
    "        \"\"\"\n",
    "        Propagate new data X through the trained network to predict\n",
    "        class label. \n",
    "        \n",
    "        \"\"\"\n",
    "        self.ForwardPropagation(self.net_weights_trained, X)\n",
    "        \n",
    "        out = self.net_output[-1]\n",
    "        if X.ndim > 1:\n",
    "            ids_max = np.argmax(out, axis = 1)\n",
    "            pred = np.array([self.labels[i] for i in ids_max]) \n",
    "        else:\n",
    "            id_max = np.argmax(out, axis = 0)\n",
    "            pred = self.labels[id_max]\n",
    "        \n",
    "        return(pred)     \n",
    "    \n",
    "    def Accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Given an array of true labels y this function measure accuracy\n",
    "        as a number of correct outputs of X divided by the total number\n",
    "        of outputs.\n",
    "        \n",
    "        \"\"\"\n",
    "        pred = self.Predict(X)\n",
    "        psize = pred.shape[0]\n",
    "        counter = 0\n",
    "        for i in range(psize):\n",
    "            if pred[i] != y[i]:\n",
    "                counter+=1\n",
    "                \n",
    "        print(\"Accuracy: {:.2f}\".format(1 - counter/psize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(X, y, [400, 25, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model was trained\n"
     ]
    }
   ],
   "source": [
    "model.Fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "model.Accuracy(model.X, model.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShowPred(arr):\n",
    "    new_img = Image.fromarray(arr.reshape(20,20)*255)\n",
    "    plt.imshow(new_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digit:  7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD4CAYAAADl7fPiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABRNJREFUeJzt3D1qVVsAhuG7JcFCsBC1sBEbRUQrC7VzAnYWFnaiQqYhWKcXbRyBiGAVCFY6AROwcQIiplBU9q1u8V4jrONPctDnKQ8fJ5sEXhZksad5nv8B+M+B/X4AYLmIAhCiAIQoACEKQKzs9wPsZpom/xKB32ye52m3z50UgBAFIEQBCFEAQhSAEAUgRAEIUQBCFIAQBSBEAQhRAEIUgBAFIEQBCFEAQhSAEAUgRAEIUQBCFIAQBSBEAQhRAEIUgBAFIEQBCFEAQhSAEAUgRAEIUQBiZb8fgJ93/Pjx4e25c+eGt69evRre7uzsDG9Zbk4KQIgCEKIAhCgAIQpAiAIQogCEKAAhCkCIAhDTPM/7/QzfmKZp+R5qia2vrw9v7969O7xdW1sb3j548GB4y3KY53na7XMnBSBEAQhRAEIUgBAFIEQBCFEAQhSAEAUgvLh1SS3yMtbr168Pb1dXV4e329vbw1v+HE4KQIgCEKIAhCgAIQpAiAIQogCEKAAhCkCIAhCuOS+pCxcuDG+PHDkyvH348OHwdnNzc3jLn8NJAQhRAEIUgBAFIEQBCFEAQhSAEAUgRAEIUQDCNecldfHixeHtgQPjbX/69OmPPA5/EScFIEQBCFEAQhSAEAUgRAEIUQBCFIAQBSBEAQjXnPfQ0aNHh7fXrl0b3r5582Z4u7GxMbxlMSdPnhzeHj58eHh75syZ4e00TcPb73FSAEIUgBAFIEQBCFEAQhSAEAUgRAEIUQBCFIBwzXkPnT9/fnh7+fLl4e29e/eGt+/fvx/eLoODBw8Ob8+ePTu8PX369NDuzp07w9956tSp4e0iPn36NLzd3t7+6Z/npACEKAAhCkCIAhCiAIQoACEKQIgCEKIAhCgA4ZrzHlpZ+T2/7tevX/+W713EsWPHhreXLl0a3t66dWt4u8gbsEe9fft2eLvIFeMnT54Mbx8/fjy8/fDhw/D28+fPu37upACEKAAhCkCIAhCiAIQoACEKQIgCEKIAhCgA4ZrzHtrZ2Rnefu8K6m6+fv06vJ2maXi7trY2vL1x48bw9sqVK8PbL1++DG8fPXo0vH358uXQbmNjY/g7t7a2hrfLzEkBCFEAQhSAEAUgRAEIUQBCFIAQBSBEAQhRAMI15z106NCh4e3q6urwdpG3GN++fXt4e/Xq1eHtIp49eza8vX///vD2xYsXP/I4/I+TAhCiAIQoACEKQIgCEKIAhCgAIQpAiAIQogCEa857aGXl9/y6F3mT8sePH4e3i7zJeH19fXj7/Pnz4e0iz8uv4aQAhCgAIQpAiAIQogCEKAAhCkCIAhCiAIQoADHN87zfz/CNaZqW76F+gRMnTgxvb968Obxd5G+4ubk5vN3a2hrevnv3bnjLcpjnedrtcycFIEQBCFEAQhSAEAUgRAEIUQBCFIAQBSDcaIS/lBuNwBBRAEIUgBAFIEQBCFEAQhSAEAUgRAEIUQBCFIAQBSBEAQhRAEIUgBAFIEQBCFEAQhSAEAUgRAEIUQBCFIAQBSBEAQhRAEIUgBAFIEQBCFEAQhSAEAUgRAEIUQBCFIAQBSBEAQhRAEIUgBAFIEQBCFEAQhSAEAUgRAEIUQBCFIAQBSBEAQhRAEIUgBAFIEQBCFEAYprneb+fAVgiTgpAiAIQogCEKAAhCkCIAhCiAIQoACEKQIgCEKIAhCgAIQpAiAIQogCEKAAhCkCIAhCiAIQoACEKQIgCEKIAxL+/j5tpPSoyxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = np.random.randint(0, X.shape[0] + 1)\n",
    "pred = model.Predict(X[example])\n",
    "if pred == 10:\n",
    "    pred = 0\n",
    "print(\"Predicted digit: \", pred)\n",
    "ShowPred(X[example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
